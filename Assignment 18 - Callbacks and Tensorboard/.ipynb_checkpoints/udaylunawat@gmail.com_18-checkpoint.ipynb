{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "1. Download the data from <a href='https://drive.google.com/file/d/15dCNcmKskcFVjs7R0ElQkR61Ex53uJpM/view?usp=sharing'>here</a>\n",
    "\n",
    "2. Code the model to classify data like below image\n",
    "\n",
    "<img src='https://i.imgur.com/33ptOFy.png'>\n",
    "\n",
    "3. Write your own callback function, that has to print the micro F1 score and AUC score after each epoch.\n",
    "\n",
    "4. Save your model at every epoch if your validation accuracy is improved from previous epoch. \n",
    "\n",
    "5. you have to decay learning based on below conditions \n",
    "        Cond1. If your validation accuracy at that epoch is less than previous epoch accuracy, you have to decrese the\n",
    "               learning rate by 10%. \n",
    "        Cond2. For every 3rd epoch, decay your learning rate by 5%.\n",
    "        \n",
    "6. If you are getting any NaN values(either weigths or loss) while training, you have to terminate your training. \n",
    "\n",
    "7. You have to stop the training if your validation accuracy is not increased in last 2 epochs.\n",
    "\n",
    "8. Use tensorboard for every model and analyse your gradients. (you need to upload the screenshots for each model for evaluation)\n",
    "\n",
    "9. use cross entropy as loss function\n",
    "\n",
    "10. Try the architecture params as given below. \n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w41Y3TFENCXk"
   },
   "source": [
    "<pre>\n",
    "<b>Model-1</b>\n",
    "<pre>\n",
    "1. Use tanh as an activation for every layer except output layer.\n",
    "2. use SGD with momentum as optimizer.\n",
    "3. use RandomUniform(0,1) as initilizer.\n",
    "3. Analyze your output and training process. \n",
    "</pre>\n",
    "</pre>\n",
    "<pre>\n",
    "<b>Model-2</b>\n",
    "<pre>\n",
    "1. Use relu as an activation for every layer except output layer.\n",
    "2. use SGD with momentum as optimizer.\n",
    "3. use RandomUniform(0,1) as initilizer.\n",
    "3. Analyze your output and training process. \n",
    "</pre>\n",
    "</pre>\n",
    "<pre>\n",
    "<b>Model-3</b>\n",
    "<pre>\n",
    "1. Use relu as an activation for every layer except output layer.\n",
    "2. use SGD with momentum as optimizer.\n",
    "3. use he_uniform() as initilizer.\n",
    "3. Analyze your output and training process. \n",
    "</pre>\n",
    "</pre>\n",
    "<pre>\n",
    "<b>Model-4</b>\n",
    "<pre>\n",
    "1. Try with any values to get better accuracy/f1 score.  \n",
    "</pre>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.450564</td>\n",
       "      <td>1.074305</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.085632</td>\n",
       "      <td>0.967682</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.117326</td>\n",
       "      <td>0.971521</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.982179</td>\n",
       "      <td>-0.380408</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.720352</td>\n",
       "      <td>0.955850</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         f1        f2  label\n",
       "0  0.450564  1.074305    0.0\n",
       "1  0.085632  0.967682    0.0\n",
       "2  0.117326  0.971521    1.0\n",
       "3  0.982179 -0.380408    0.0\n",
       "4 -0.720352  0.955850    0.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    10000\n",
       "0.0    10000\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = data.drop('label', axis = 1)\n",
    "y = data['label'].values\n",
    "# y = y.astype(int)\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "y = to_categorical(y, num_classes = 2)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, shuffle = True, stratify = y, random_state = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16000, 2)\n",
      "(4000, 2)\n",
      "(16000, 2)\n",
      "(4000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "tf.enable_eager_execution()\n",
    "import keras, os\n",
    "\n",
    "from tensorflow.keras.layers import Dense,Input,Activation\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping\n",
    "import random as rn\n",
    "\n",
    "import datetime, math\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/tutorials/structured_data/imbalanced_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate Micro F1 score and AUC score for every epoch\n",
    "\n",
    "class LossHistory(tf.keras.callbacks.Callback):\n",
    "    \n",
    "    def on_train_begin(self, logs={}):\n",
    "        ## on begin of training, we are creating a instance varible called history\n",
    "        ## it is a dict with keys [loss, acc, val_loss, val_acc]\n",
    "        self.history={'loss': [], 'acc': [], 'AUC': [], 'f1': [], 'val_loss': [], 'val_acc': []}\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        loss = logs.get('loss')\n",
    "        \n",
    "        # Terminating training if NaN values occur while training\n",
    "        if loss is not None:\n",
    "            if np.isnan(loss) or np.isinf(loss):\n",
    "                print(\"Invalid loss and terminated at epoch {}\".format(epoch))\n",
    "                self.model.stop_training = True\n",
    "                \n",
    "        ## on end of each epoch, we will get logs and update the self.history dict\n",
    "        self.history['loss'].append(logs.get('loss'))\n",
    "        self.history['acc'].append(logs.get('acc'))\n",
    "        self.history['f1'].append(logs.get('f1'))\n",
    "        self.history['AUC'].append(logs.get('AUC'))\n",
    "        \n",
    "        if logs.get('val_loss', -1) != -1:\n",
    "            self.history['val_loss'].append(logs.get('val_loss'))\n",
    "        if logs.get('val_acc', -1) != -1:\n",
    "            self.history['val_acc'].append(logs.get('val_acc'))\n",
    "            \n",
    "history_own=LossHistory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/a/45305384/9292995\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "<b>Model-1</b>\n",
    "<pre>\n",
    "1. Use tanh as an activation for every layer except output layer.\n",
    "2. use SGD with momentum as optimizer.\n",
    "3. use RandomUniform(0,1) as initilizer.\n",
    "3. Analyze your output and training process. \n",
    "</pre>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Input layer\n",
    "# input_layer = Input(shape=(2,))\n",
    "# #Dense hidden layer\n",
    "# layer1 = Dense(32,activation='tanh',kernel_initializer = tf.keras.initializers.RandomUniform(0, 1))(input_layer)\n",
    "# #Dense hidden layer\n",
    "# layer2 = Dense(16,activation='tanh',kernel_initializer = tf.keras.initializers.RandomUniform(0, 1))(layer1)\n",
    "# #Dense hidden layer\n",
    "# layer3 = Dense(8,activation='tanh',kernel_initializer = tf.keras.initializers.RandomUniform(0, 1))(layer2)\n",
    "# #Dense hidden layer\n",
    "# layer4 = Dense(4,activation='tanh',kernel_initializer = tf.keras.initializers.RandomUniform(0, 1))(layer3)\n",
    "# #Dense hidden layer\n",
    "# layer5 = Dense(2,activation='tanh',kernel_initializer = tf.keras.initializers.RandomUniform(0, 1))(layer4)\n",
    "# #output layer\n",
    "# output = Dense(2, activation='sigmoid',kernel_initializer = tf.keras.initializers.RandomUniform(0, 1))(layer5)\n",
    "\n",
    "# #Creating a model\n",
    "# model = Model(inputs=input_layer,outputs=output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save your model at every epoch if your validation accuracy is improved from previous epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving model\n",
    "os.makedirs(\"model_save\",exist_ok=True)\n",
    "\n",
    "filepath=\"model_save/weights-{epoch:02d}-{val_acc:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_acc',  verbose=0, save_best_only=True, mode='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Drop based Learning Rate](https://machinelearningmastery.com/using-learning-rate-schedules-deep-learning-models-python-keras/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop-Based Learning Rate Decay\n",
    "def step_decay(epoch, lrate, accuracy, last_accuracy):\n",
    "    \n",
    "    # If accuracy decreases, drop learning rate by 10%\n",
    "    if accuracy < last_accuracy:\n",
    "        lrate = lrate*(1-0.1)\n",
    "    \n",
    "    #For every 3rd epoch decay lr by 5%\n",
    "    if epoch!=0 and epoch % 3 == 0:\n",
    "        drop = 0.05\n",
    "        lrate = lrate * (1- 0.05)\n",
    "    \n",
    "    return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateScheduler(tf.keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, lr_decay):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        self.decay = lr_decay\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.history={'loss': [],'acc': [],'val_loss': [],'val_acc': []}\n",
    "        \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if epoch>2:\n",
    "            self.last_accuracy = self.history['val_acc'][-1]\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        ## on end of each epoch, we will get logs and update the self.history dict\n",
    "        self.history['loss'].append(logs.get('loss'))\n",
    "        self.history['acc'].append(logs.get('acc'))\n",
    "        if logs.get('val_loss', -1) != -1:\n",
    "            self.history['val_loss'].append(logs.get('val_loss'))\n",
    "        if logs.get('val_acc', -1) != -1:\n",
    "            self.history['val_acc'].append(logs.get('val_acc'))\n",
    "        \n",
    "        print(logs)\n",
    "        self.accuracy = logs.get('val_acc')\n",
    "    \n",
    "        \n",
    "        # Get the current learning rate from model's optimizer.\n",
    "        # https://stackoverflow.com/a/49911808/9292995\n",
    "        lrate = float(tf.keras.backend.get_value(self.model.optimizer.lr))\n",
    "        \n",
    "        if epoch>2:\n",
    "            # Call schedule function to get the scheduled learning rate.\n",
    "            updated_lr = self.decay(epoch, lrate, self.accuracy, self.last_accuracy)\n",
    "#             if self.accuracy < self.last_accuracy: print(\"\\nAccuracy decreased!!, Dropping Learning rate by 10%\", updated_lr)\n",
    "#             if epoch!=0 and epoch % 3 == 0: print(\"\\n3rd Epoch!! Dropping Learning rate by 5%\", updated_lr)\n",
    "\n",
    "            # Set the value back to the optimizer before this epoch starts\n",
    "            tf.keras.backend.set_value(self.model.optimizer.lr, updated_lr)\n",
    "        \n",
    "        \n",
    "lrate = LearningRateScheduler(step_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TerminateNaN(tf.keras.callbacks.Callback):\n",
    "        \n",
    "#     def on_epoch_end(self, epoch, logs={}):\n",
    "#         loss = logs.get('loss')\n",
    "#         if loss is not None:\n",
    "#             if np.isnan(loss) or np.isinf(loss):\n",
    "#                 print(\"Invalid loss and terminated at epoch {}\".format(epoch))\n",
    "#                 self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Callbacks\n",
    "history_own = LossHistory()\n",
    "\n",
    "# https://stackoverflow.com/a/60371671/9292995\n",
    "log_dir=\"logs\\\\fit\\\\\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir,write_graph=True)\n",
    "earlystop = EarlyStopping(monitor='val_acc', min_delta=0.0005, patience=2, verbose=1)\n",
    "# optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
    "\n",
    "# model.compile(optimizer,\n",
    "# #       loss=\"binary_crossentropy\",\n",
    "#       loss = \"categorical_crossentropy\",\n",
    "#       metrics=['AUC', 'accuracy', f1])\n",
    "\n",
    "\n",
    "# model.fit(X_train,y_train,epochs=50, validation_data=(X_test,y_test), batch_size=16, \n",
    "#           callbacks=[history_own, checkpoint, lrate, \n",
    "# #                      earlystop, \n",
    "#                      tensorboard_callback]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w41Y3TFENCXk"
   },
   "source": [
    "<pre>\n",
    "<b>Model-1</b>\n",
    "<pre>\n",
    "1. Use tanh as an activation for every layer except output layer.\n",
    "2. use SGD with momentum as optimizer.\n",
    "3. use RandomUniform(0,1) as initilizer.\n",
    "3. Analyze your output and training process. \n",
    "</pre>\n",
    "</pre>\n",
    "<pre>\n",
    "<b>Model-2</b>\n",
    "<pre>\n",
    "1. Use relu as an activation for every layer except output layer.\n",
    "2. use SGD with momentum as optimizer.\n",
    "3. use RandomUniform(0,1) as initilizer.\n",
    "3. Analyze your output and training process. \n",
    "</pre>\n",
    "</pre>\n",
    "<pre>\n",
    "<b>Model-3</b>\n",
    "<pre>\n",
    "1. Use relu as an activation for every layer except output layer.\n",
    "2. use SGD with momentum as optimizer.\n",
    "3. use he_uniform() as initilizer.\n",
    "3. Analyze your output and training process. \n",
    "</pre>\n",
    "</pre>\n",
    "<pre>\n",
    "<b>Model-4</b>\n",
    "<pre>\n",
    "1. Try with any values to get better accuracy/f1 score.  \n",
    "</pre>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(activation, kernel_initializer):\n",
    "\n",
    "    #Input layer\n",
    "    input_layer = Input(shape=(2,))\n",
    "    #Dense hidden layer\n",
    "    layer1 = Dense(2, activation, kernel_initializer)(input_layer)\n",
    "    #Dense hidden layer\n",
    "    layer2 = Dense(2, activation, kernel_initializer)(layer1)\n",
    "    #Dense hidden layer\n",
    "    layer3 = Dense(2, activation, kernel_initializer)(layer2)\n",
    "    #Dense hidden layer\n",
    "    layer4 = Dense(4, activation, kernel_initializer)(layer3)\n",
    "    #Dense hidden layer\n",
    "    layer5 = Dense(2, activation, kernel_initializer)(layer4)\n",
    "    #output layer\n",
    "    output = Dense(2, 'sigmoid', kernel_initializer)(layer5)\n",
    "\n",
    "    #Creating a model\n",
    "    model = Model(inputs=input_layer,outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = 'tanh'\n",
    "kernel_initializer = tf.keras.initializers.RandomUniform(0, 1)\n",
    "model1 = model(activation, kernel_initializer)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/50\n",
      "15936/16000 [============================>.] - ETA: 0s - loss: 0.6492 - auc_10: 0.6172 - acc: 0.6250 - f1: 0.5456{'loss': 0.6493565646708012, 'auc_10': 0.617022, 'acc': 0.6248125, 'f1': 0.5455509, 'val_loss': 0.6554478642940521, 'val_auc_10': 0.6095049, 'val_acc': 0.61775, 'val_f1': 0.5396635}\n",
      "16000/16000 [==============================] - 3s 208us/sample - loss: 0.6494 - auc_10: 0.6170 - acc: 0.6248 - f1: 0.5456 - val_loss: 0.6554 - val_auc_10: 0.6095 - val_acc: 0.6177 - val_f1: 0.5397\n",
      "Epoch 2/50\n",
      "15840/16000 [============================>.] - ETA: 0s - loss: 0.6498 - auc_10: 0.6165 - acc: 0.6244 - f1: 0.5467{'loss': 0.6493750103116035, 'auc_10': 0.6170684, 'acc': 0.625, 'f1': 0.54710805, 'val_loss': 0.6555279762744903, 'val_auc_10': 0.6098485, 'val_acc': 0.619, 'val_f1': 0.5424636}\n",
      "16000/16000 [==============================] - 3s 170us/sample - loss: 0.6494 - auc_10: 0.6171 - acc: 0.6250 - f1: 0.5471 - val_loss: 0.6555 - val_auc_10: 0.6098 - val_acc: 0.6190 - val_f1: 0.5425\n",
      "Epoch 3/50\n",
      "15728/16000 [============================>.] - ETA: 0s - loss: 0.6493 - auc_10: 0.6168 - acc: 0.6257 - f1: 0.5470{'loss': 0.649306718736887, 'auc_10': 0.6168779, 'acc': 0.6256875, 'f1': 0.5471027, 'val_loss': 0.6557745660543441, 'val_auc_10': 0.6107237, 'val_acc': 0.6195, 'val_f1': 0.54527205}\n",
      "16000/16000 [==============================] - 3s 173us/sample - loss: 0.6493 - auc_10: 0.6169 - acc: 0.6257 - f1: 0.5471 - val_loss: 0.6558 - val_auc_10: 0.6107 - val_acc: 0.6195 - val_f1: 0.5453\n",
      "Epoch 4/50\n",
      "15680/16000 [============================>.] - ETA: 0s - loss: 0.6489 - auc_10: 0.6184 - acc: 0.6250 - f1: 0.5491{'loss': 0.6492257664501667, 'auc_10': 0.61798793, 'acc': 0.6245625, 'f1': 0.5486077, 'val_loss': 0.6555503787994384, 'val_auc_10': 0.60927534, 'val_acc': 0.61725, 'val_f1': 0.5393465}\n",
      "16000/16000 [==============================] - 3s 181us/sample - loss: 0.6492 - auc_10: 0.6180 - acc: 0.6246 - f1: 0.5486 - val_loss: 0.6556 - val_auc_10: 0.6093 - val_acc: 0.6173 - val_f1: 0.5393\n",
      "Epoch 5/50\n",
      "15824/16000 [============================>.] - ETA: 0s - loss: 0.6494 - auc_10: 0.6171 - acc: 0.6253 - f1: 0.5476{'loss': 0.6492613109648228, 'auc_10': 0.61718535, 'acc': 0.625125, 'f1': 0.54764044, 'val_loss': 0.6555755701065064, 'val_auc_10': 0.6096084, 'val_acc': 0.61725, 'val_f1': 0.5411704}\n",
      "16000/16000 [==============================] - 3s 176us/sample - loss: 0.6493 - auc_10: 0.6172 - acc: 0.6251 - f1: 0.5476 - val_loss: 0.6556 - val_auc_10: 0.6096 - val_acc: 0.6173 - val_f1: 0.5412\n",
      "Epoch 6/50\n",
      "15792/16000 [============================>.] - ETA: 0s - loss: 0.6491 - auc_10: 0.6170 - acc: 0.6249 - f1: 0.5451{'loss': 0.6492090232670307, 'auc_10': 0.6169348, 'acc': 0.6249375, 'f1': 0.54498476, 'val_loss': 0.6554532594680786, 'val_auc_10': 0.609469, 'val_acc': 0.61775, 'val_f1': 0.54241264}\n",
      "16000/16000 [==============================] - 3s 168us/sample - loss: 0.6492 - auc_10: 0.6169 - acc: 0.6249 - f1: 0.5450 - val_loss: 0.6555 - val_auc_10: 0.6095 - val_acc: 0.6177 - val_f1: 0.5424\n",
      "Epoch 7/50\n",
      "15984/16000 [============================>.] - ETA: 0s - loss: 0.6491 - auc_10: 0.6173 - acc: 0.6241 - f1: 0.5471{'loss': 0.6491752336621285, 'auc_10': 0.6171974, 'acc': 0.624, 'f1': 0.5470977, 'val_loss': 0.6554363167285919, 'val_auc_10': 0.6094131, 'val_acc': 0.618, 'val_f1': 0.5404787}\n",
      "16000/16000 [==============================] - 3s 169us/sample - loss: 0.6492 - auc_10: 0.6172 - acc: 0.6240 - f1: 0.5471 - val_loss: 0.6554 - val_auc_10: 0.6094 - val_acc: 0.6180 - val_f1: 0.5405\n",
      "Epoch 8/50\n",
      "15824/16000 [============================>.] - ETA: 0s - loss: 0.6492 - auc_10: 0.6170 - acc: 0.6246 - f1: 0.5474{'loss': 0.6492089126706123, 'auc_10': 0.61701363, 'acc': 0.6248125, 'f1': 0.5466513, 'val_loss': 0.6555760395526886, 'val_auc_10': 0.6107558, 'val_acc': 0.61925, 'val_f1': 0.5442805}\n",
      "16000/16000 [==============================] - 3s 170us/sample - loss: 0.6492 - auc_10: 0.6170 - acc: 0.6248 - f1: 0.5467 - val_loss: 0.6556 - val_auc_10: 0.6108 - val_acc: 0.6192 - val_f1: 0.5443\n",
      "Epoch 9/50\n",
      "15808/16000 [============================>.] - ETA: 0s - loss: 0.6490 - auc_10: 0.6177 - acc: 0.6246 - f1: 0.5482{'loss': 0.6492451970875264, 'auc_10': 0.61729705, 'acc': 0.6239375, 'f1': 0.54780895, 'val_loss': 0.6553580148220062, 'val_auc_10': 0.6105542, 'val_acc': 0.62, 'val_f1': 0.5417619}\n",
      "16000/16000 [==============================] - 3s 175us/sample - loss: 0.6492 - auc_10: 0.6173 - acc: 0.6239 - f1: 0.5478 - val_loss: 0.6554 - val_auc_10: 0.6106 - val_acc: 0.6200 - val_f1: 0.5418\n",
      "Epoch 10/50\n",
      "15696/16000 [============================>.] - ETA: 0s - loss: 0.6491 - auc_10: 0.6173 - acc: 0.6231 - f1: 0.5468{'loss': 0.6490855985879898, 'auc_10': 0.6174928, 'acc': 0.6235, 'f1': 0.5468953, 'val_loss': 0.655462739944458, 'val_auc_10': 0.6104077, 'val_acc': 0.619, 'val_f1': 0.5422299}\n",
      "16000/16000 [==============================] - 3s 201us/sample - loss: 0.6491 - auc_10: 0.6175 - acc: 0.6235 - f1: 0.5469 - val_loss: 0.6555 - val_auc_10: 0.6104 - val_acc: 0.6190 - val_f1: 0.5422\n",
      "Epoch 11/50\n",
      "15712/16000 [============================>.] - ETA: 0s - loss: 0.6496 - auc_10: 0.6168 - acc: 0.6240 - f1: 0.5449{'loss': 0.6491900248229504, 'auc_10': 0.6171923, 'acc': 0.624375, 'f1': 0.54523253, 'val_loss': 0.6553380992412567, 'val_auc_10': 0.6104434, 'val_acc': 0.61925, 'val_f1': 0.54094076}\n",
      "16000/16000 [==============================] - 2s 149us/sample - loss: 0.6492 - auc_10: 0.6172 - acc: 0.6244 - f1: 0.5452 - val_loss: 0.6553 - val_auc_10: 0.6104 - val_acc: 0.6192 - val_f1: 0.5409\n",
      "Epoch 12/50\n",
      "15808/16000 [============================>.] - ETA: 0s - loss: 0.6491 - auc_10: 0.6175 - acc: 0.6245 - f1: 0.5476{'loss': 0.6491793910264969, 'auc_10': 0.61738193, 'acc': 0.6245625, 'f1': 0.5472605, 'val_loss': 0.6553953408002854, 'val_auc_10': 0.60990834, 'val_acc': 0.618, 'val_f1': 0.5407142}\n",
      "16000/16000 [==============================] - 3s 179us/sample - loss: 0.6492 - auc_10: 0.6174 - acc: 0.6246 - f1: 0.5473 - val_loss: 0.6554 - val_auc_10: 0.6099 - val_acc: 0.6180 - val_f1: 0.5407\n",
      "Epoch 13/50\n",
      "15904/16000 [============================>.] - ETA: 0s - loss: 0.6489 - auc_10: 0.6174 - acc: 0.6252 - f1: 0.5469{'loss': 0.6491101071834564, 'auc_10': 0.6172321, 'acc': 0.6249375, 'f1': 0.54676926, 'val_loss': 0.6554706205129623, 'val_auc_10': 0.61104095, 'val_acc': 0.619, 'val_f1': 0.5445751}\n",
      "16000/16000 [==============================] - 3s 188us/sample - loss: 0.6491 - auc_10: 0.6172 - acc: 0.6249 - f1: 0.5468 - val_loss: 0.6555 - val_auc_10: 0.6110 - val_acc: 0.6190 - val_f1: 0.5446\n",
      "Epoch 14/50\n",
      "15952/16000 [============================>.] - ETA: 0s - loss: 0.6492 - auc_10: 0.6174 - acc: 0.6254 - f1: 0.5470{'loss': 0.6491695183813572, 'auc_10': 0.61750364, 'acc': 0.6254375, 'f1': 0.54713446, 'val_loss': 0.6554202907085419, 'val_auc_10': 0.6105341, 'val_acc': 0.619, 'val_f1': 0.5421001}\n",
      "16000/16000 [==============================] - 3s 189us/sample - loss: 0.6492 - auc_10: 0.6175 - acc: 0.6254 - f1: 0.5471 - val_loss: 0.6554 - val_auc_10: 0.6105 - val_acc: 0.6190 - val_f1: 0.5421\n",
      "Epoch 15/50\n",
      "15680/16000 [============================>.] - ETA: 0s - loss: 0.6488 - auc_10: 0.6177 - acc: 0.6255 - f1: 0.5487{'loss': 0.6491212869286537, 'auc_10': 0.6171522, 'acc': 0.6254375, 'f1': 0.5480634, 'val_loss': 0.6555588626861573, 'val_auc_10': 0.61055136, 'val_acc': 0.61875, 'val_f1': 0.54289985}\n",
      "16000/16000 [==============================] - 3s 193us/sample - loss: 0.6491 - auc_10: 0.6172 - acc: 0.6254 - f1: 0.5481 - val_loss: 0.6556 - val_auc_10: 0.6106 - val_acc: 0.6187 - val_f1: 0.5429\n",
      "Epoch 16/50\n",
      "15840/16000 [============================>.] - ETA: 0s - loss: 0.6493 - auc_10: 0.6172 - acc: 0.6234 - f1: 0.5469{'loss': 0.649154069930315, 'auc_10': 0.61735964, 'acc': 0.6238125, 'f1': 0.54688084, 'val_loss': 0.6555076286792755, 'val_auc_10': 0.61017805, 'val_acc': 0.61925, 'val_f1': 0.54214525}\n",
      "16000/16000 [==============================] - 3s 181us/sample - loss: 0.6492 - auc_10: 0.6174 - acc: 0.6238 - f1: 0.5469 - val_loss: 0.6555 - val_auc_10: 0.6102 - val_acc: 0.6192 - val_f1: 0.5421\n",
      "Epoch 17/50\n",
      "15664/16000 [============================>.] - ETA: 0s - loss: 0.6495 - auc_10: 0.6169 - acc: 0.6242 - f1: 0.5485{'loss': 0.649119825065136, 'auc_10': 0.6173599, 'acc': 0.624875, 'f1': 0.5487884, 'val_loss': 0.6554657715559006, 'val_auc_10': 0.61049414, 'val_acc': 0.61975, 'val_f1': 0.5423991}\n",
      "16000/16000 [==============================] - 3s 162us/sample - loss: 0.6491 - auc_10: 0.6174 - acc: 0.6249 - f1: 0.5488 - val_loss: 0.6555 - val_auc_10: 0.6105 - val_acc: 0.6198 - val_f1: 0.5424\n",
      "Epoch 18/50\n",
      "15616/16000 [============================>.] - ETA: 0s - loss: 0.6489 - auc_10: 0.6179 - acc: 0.6249 - f1: 0.5480{'loss': 0.6490130771994591, 'auc_10': 0.6177438, 'acc': 0.6246875, 'f1': 0.54759085, 'val_loss': 0.6553245041370391, 'val_auc_10': 0.60961014, 'val_acc': 0.61775, 'val_f1': 0.5403905}\n",
      "16000/16000 [==============================] - 3s 172us/sample - loss: 0.6490 - auc_10: 0.6177 - acc: 0.6247 - f1: 0.5476 - val_loss: 0.6553 - val_auc_10: 0.6096 - val_acc: 0.6177 - val_f1: 0.5404\n",
      "Epoch 19/50\n",
      "15664/16000 [============================>.] - ETA: 0s - loss: 0.6492 - auc_10: 0.6171 - acc: 0.6251 - f1: 0.5443{'loss': 0.6490514597594738, 'auc_10': 0.61723655, 'acc': 0.6255, 'f1': 0.54491454, 'val_loss': 0.6554475150108338, 'val_auc_10': 0.6108769, 'val_acc': 0.62, 'val_f1': 0.5441321}\n",
      "16000/16000 [==============================] - 3s 169us/sample - loss: 0.6491 - auc_10: 0.6172 - acc: 0.6255 - f1: 0.5449 - val_loss: 0.6554 - val_auc_10: 0.6109 - val_acc: 0.6200 - val_f1: 0.5441\n",
      "Epoch 20/50\n",
      "15712/16000 [============================>.] - ETA: 0s - loss: 0.6495 - auc_10: 0.6170 - acc: 0.6246 - f1: 0.5457{'loss': 0.6491049512326718, 'auc_10': 0.6176896, 'acc': 0.6249375, 'f1': 0.5469532, 'val_loss': 0.6554780359268189, 'val_auc_10': 0.61060846, 'val_acc': 0.62025, 'val_f1': 0.5427735}\n",
      "16000/16000 [==============================] - 3s 172us/sample - loss: 0.6491 - auc_10: 0.6177 - acc: 0.6249 - f1: 0.5470 - val_loss: 0.6555 - val_auc_10: 0.6106 - val_acc: 0.6202 - val_f1: 0.5428\n",
      "Epoch 21/50\n",
      "15824/16000 [============================>.] - ETA: 0s - loss: 0.6491 - auc_10: 0.6172 - acc: 0.6243 - f1: 0.5469{'loss': 0.6490375220179557, 'auc_10': 0.6174495, 'acc': 0.624375, 'f1': 0.5468665, 'val_loss': 0.6554045361280442, 'val_auc_10': 0.61073744, 'val_acc': 0.62025, 'val_f1': 0.5431092}\n",
      "16000/16000 [==============================] - 3s 175us/sample - loss: 0.6490 - auc_10: 0.6174 - acc: 0.6244 - f1: 0.5469 - val_loss: 0.6554 - val_auc_10: 0.6107 - val_acc: 0.6202 - val_f1: 0.5431\n",
      "Epoch 22/50\n",
      "15664/16000 [============================>.] - ETA: 0s - loss: 0.6494 - auc_10: 0.6171 - acc: 0.6246 - f1: 0.5469{'loss': 0.6490670393407345, 'auc_10': 0.6174077, 'acc': 0.625625, 'f1': 0.5468142, 'val_loss': 0.6554469336271286, 'val_auc_10': 0.6109506, 'val_acc': 0.62, 'val_f1': 0.54171044}\n",
      "16000/16000 [==============================] - 3s 182us/sample - loss: 0.6491 - auc_10: 0.6174 - acc: 0.6256 - f1: 0.5468 - val_loss: 0.6554 - val_auc_10: 0.6110 - val_acc: 0.6200 - val_f1: 0.5417\n",
      "Epoch 23/50\n",
      "15648/16000 [============================>.] - ETA: 0s - loss: 0.6489 - auc_10: 0.6180 - acc: 0.6254 - f1: 0.5483{'loss': 0.6490499339103699, 'auc_10': 0.61780214, 'acc': 0.625, 'f1': 0.5477338, 'val_loss': 0.6553755732774734, 'val_auc_10': 0.61036587, 'val_acc': 0.6185, 'val_f1': 0.54019964}\n",
      "16000/16000 [==============================] - 3s 175us/sample - loss: 0.6490 - auc_10: 0.6178 - acc: 0.6250 - f1: 0.5477 - val_loss: 0.6554 - val_auc_10: 0.6104 - val_acc: 0.6185 - val_f1: 0.5402\n",
      "Epoch 24/50\n",
      "15664/16000 [============================>.] - ETA: 0s - loss: 0.6493 - auc_10: 0.6171 - acc: 0.6247 - f1: 0.5457{'loss': 0.6490272091627121, 'auc_10': 0.6175709, 'acc': 0.6253125, 'f1': 0.5463602, 'val_loss': 0.6555301098823547, 'val_auc_10': 0.6108711, 'val_acc': 0.6195, 'val_f1': 0.5421738}\n",
      "16000/16000 [==============================] - 3s 184us/sample - loss: 0.6490 - auc_10: 0.6176 - acc: 0.6253 - f1: 0.5464 - val_loss: 0.6555 - val_auc_10: 0.6109 - val_acc: 0.6195 - val_f1: 0.5422\n",
      "Epoch 25/50\n",
      "15872/16000 [============================>.] - ETA: 0s - loss: 0.6488 - auc_10: 0.6179 - acc: 0.6253 - f1: 0.5465{'loss': 0.6490357491672039, 'auc_10': 0.6175048, 'acc': 0.625125, 'f1': 0.5465692, 'val_loss': 0.6554592578411103, 'val_auc_10': 0.61046755, 'val_acc': 0.619, 'val_f1': 0.5422192}\n",
      "16000/16000 [==============================] - 3s 200us/sample - loss: 0.6490 - auc_10: 0.6175 - acc: 0.6251 - f1: 0.5466 - val_loss: 0.6555 - val_auc_10: 0.6105 - val_acc: 0.6190 - val_f1: 0.5422\n",
      "Epoch 26/50\n",
      "15872/16000 [============================>.] - ETA: 0s - loss: 0.6490 - auc_10: 0.6175 - acc: 0.6251 - f1: 0.5463{'loss': 0.6490052337050438, 'auc_10': 0.6174337, 'acc': 0.624875, 'f1': 0.54632115, 'val_loss': 0.6555078965425492, 'val_auc_10': 0.61085975, 'val_acc': 0.61975, 'val_f1': 0.5438512}\n",
      "16000/16000 [==============================] - 3s 201us/sample - loss: 0.6490 - auc_10: 0.6174 - acc: 0.6249 - f1: 0.5463 - val_loss: 0.6555 - val_auc_10: 0.6109 - val_acc: 0.6198 - val_f1: 0.5439\n",
      "Epoch 27/50\n",
      "15984/16000 [============================>.] - ETA: 0s - loss: 0.6490 - auc_10: 0.6178 - acc: 0.6242 - f1: 0.5466{'loss': 0.6490372353494167, 'auc_10': 0.6177372, 'acc': 0.6241875, 'f1': 0.5465776, 'val_loss': 0.6554429783821106, 'val_auc_10': 0.6104234, 'val_acc': 0.61975, 'val_f1': 0.5401221}\n",
      "16000/16000 [==============================] - 3s 187us/sample - loss: 0.6490 - auc_10: 0.6177 - acc: 0.6242 - f1: 0.5466 - val_loss: 0.6554 - val_auc_10: 0.6104 - val_acc: 0.6198 - val_f1: 0.5401\n",
      "Epoch 28/50\n",
      "15904/16000 [============================>.] - ETA: 0s - loss: 0.6495 - auc_10: 0.6169 - acc: 0.6240 - f1: 0.5460{'loss': 0.6489871301651001, 'auc_10': 0.6175468, 'acc': 0.6246875, 'f1': 0.5466546, 'val_loss': 0.6553700515031815, 'val_auc_10': 0.61023027, 'val_acc': 0.619, 'val_f1': 0.5406504}\n",
      "16000/16000 [==============================] - 3s 174us/sample - loss: 0.6490 - auc_10: 0.6175 - acc: 0.6247 - f1: 0.5467 - val_loss: 0.6554 - val_auc_10: 0.6102 - val_acc: 0.6190 - val_f1: 0.5407\n",
      "Epoch 29/50\n",
      "15696/16000 [============================>.] - ETA: 0s - loss: 0.6494 - auc_10: 0.6169 - acc: 0.6249 - f1: 0.5456{'loss': 0.6489806363880635, 'auc_10': 0.61734426, 'acc': 0.6255, 'f1': 0.5461287, 'val_loss': 0.6554858864545822, 'val_auc_10': 0.6106724, 'val_acc': 0.61975, 'val_f1': 0.54279727}\n",
      "16000/16000 [==============================] - 3s 171us/sample - loss: 0.6490 - auc_10: 0.6173 - acc: 0.6255 - f1: 0.5461 - val_loss: 0.6555 - val_auc_10: 0.6107 - val_acc: 0.6198 - val_f1: 0.5428\n",
      "Epoch 30/50\n",
      "15824/16000 [============================>.] - ETA: 0s - loss: 0.6490 - auc_10: 0.6176 - acc: 0.6250 - f1: 0.5459{'loss': 0.6489507589042187, 'auc_10': 0.6175629, 'acc': 0.6249375, 'f1': 0.5463156, 'val_loss': 0.6555933133363724, 'val_auc_10': 0.61116636, 'val_acc': 0.619, 'val_f1': 0.5436586}\n",
      "16000/16000 [==============================] - 3s 180us/sample - loss: 0.6490 - auc_10: 0.6176 - acc: 0.6249 - f1: 0.5463 - val_loss: 0.6556 - val_auc_10: 0.6112 - val_acc: 0.6190 - val_f1: 0.5437\n",
      "Epoch 31/50\n",
      "15984/16000 [============================>.] - ETA: 0s - loss: 0.6490 - auc_10: 0.6177 - acc: 0.6252 - f1: 0.5483{'loss': 0.648997756421566, 'auc_10': 0.61769646, 'acc': 0.625125, 'f1': 0.54837334, 'val_loss': 0.6554798933267594, 'val_auc_10': 0.61069316, 'val_acc': 0.6195, 'val_f1': 0.54235935}\n",
      "16000/16000 [==============================] - 3s 164us/sample - loss: 0.6490 - auc_10: 0.6177 - acc: 0.6251 - f1: 0.5484 - val_loss: 0.6555 - val_auc_10: 0.6107 - val_acc: 0.6195 - val_f1: 0.5424\n",
      "Epoch 32/50\n",
      "15920/16000 [============================>.] - ETA: 0s - loss: 0.6487 - auc_10: 0.6182 - acc: 0.6249 - f1: 0.5476{'loss': 0.6489764560759067, 'auc_10': 0.6178179, 'acc': 0.6245, 'f1': 0.546972, 'val_loss': 0.6554261441230774, 'val_auc_10': 0.6104926, 'val_acc': 0.6195, 'val_f1': 0.54067755}\n",
      "16000/16000 [==============================] - 3s 171us/sample - loss: 0.6490 - auc_10: 0.6178 - acc: 0.6245 - f1: 0.5470 - val_loss: 0.6554 - val_auc_10: 0.6105 - val_acc: 0.6195 - val_f1: 0.5407\n",
      "Epoch 33/50\n",
      "15600/16000 [============================>.] - ETA: 0s - loss: 0.6492 - auc_10: 0.6173 - acc: 0.6238 - f1: 0.5464{'loss': 0.6489773198068142, 'auc_10': 0.6175537, 'acc': 0.624375, 'f1': 0.5465809, 'val_loss': 0.6554368674755097, 'val_auc_10': 0.6104691, 'val_acc': 0.61925, 'val_f1': 0.5418491}\n",
      "16000/16000 [==============================] - 3s 157us/sample - loss: 0.6490 - auc_10: 0.6176 - acc: 0.6244 - f1: 0.5466 - val_loss: 0.6554 - val_auc_10: 0.6105 - val_acc: 0.6192 - val_f1: 0.5418\n",
      "Epoch 34/50\n",
      "15920/16000 [============================>.] - ETA: 0s - loss: 0.6492 - auc_10: 0.6173 - acc: 0.6244 - f1: 0.5466{'loss': 0.6489563672244549, 'auc_10': 0.6176479, 'acc': 0.6245625, 'f1': 0.54693425, 'val_loss': 0.6554653253555298, 'val_auc_10': 0.61049634, 'val_acc': 0.619, 'val_f1': 0.54392743}\n",
      "16000/16000 [==============================] - 3s 169us/sample - loss: 0.6490 - auc_10: 0.6176 - acc: 0.6246 - f1: 0.5469 - val_loss: 0.6555 - val_auc_10: 0.6105 - val_acc: 0.6190 - val_f1: 0.5439\n",
      "Epoch 35/50\n",
      "15872/16000 [============================>.] - ETA: 0s - loss: 0.6490 - auc_10: 0.6177 - acc: 0.6244 - f1: 0.5466{'loss': 0.6489477643370628, 'auc_10': 0.617718, 'acc': 0.6245625, 'f1': 0.54658693, 'val_loss': 0.6554369728565216, 'val_auc_10': 0.61038816, 'val_acc': 0.61975, 'val_f1': 0.5406312}\n",
      "16000/16000 [==============================] - 3s 207us/sample - loss: 0.6489 - auc_10: 0.6177 - acc: 0.6246 - f1: 0.5466 - val_loss: 0.6554 - val_auc_10: 0.6104 - val_acc: 0.6198 - val_f1: 0.5406\n",
      "Epoch 36/50\n",
      "15952/16000 [============================>.] - ETA: 0s - loss: 0.6491 - auc_10: 0.6173 - acc: 0.6242 - f1: 0.5469{'loss': 0.64895812317729, 'auc_10': 0.61759377, 'acc': 0.624625, 'f1': 0.5468555, 'val_loss': 0.6554292228221893, 'val_auc_10': 0.61045164, 'val_acc': 0.61925, 'val_f1': 0.54097664}\n",
      "16000/16000 [==============================] - 3s 190us/sample - loss: 0.6490 - auc_10: 0.6176 - acc: 0.6246 - f1: 0.5469 - val_loss: 0.6554 - val_auc_10: 0.6105 - val_acc: 0.6192 - val_f1: 0.5410\n",
      "Epoch 37/50\n",
      "15760/16000 [============================>.] - ETA: 0s - loss: 0.6492 - auc_10: 0.6172 - acc: 0.6245 - f1: 0.5461{'loss': 0.648946300804615, 'auc_10': 0.61759764, 'acc': 0.6249375, 'f1': 0.546033, 'val_loss': 0.6554574209451676, 'val_auc_10': 0.6106784, 'val_acc': 0.62, 'val_f1': 0.5405509}\n",
      "16000/16000 [==============================] - 3s 166us/sample - loss: 0.6489 - auc_10: 0.6176 - acc: 0.6249 - f1: 0.5460 - val_loss: 0.6555 - val_auc_10: 0.6107 - val_acc: 0.6200 - val_f1: 0.5406\n",
      "Epoch 38/50\n",
      "15888/16000 [============================>.] - ETA: 0s - loss: 0.6489 - auc_10: 0.6178 - acc: 0.6250 - f1: 0.5470{'loss': 0.6489444797337055, 'auc_10': 0.61775166, 'acc': 0.624875, 'f1': 0.54689854, 'val_loss': 0.6554410701990128, 'val_auc_10': 0.61059105, 'val_acc': 0.61975, 'val_f1': 0.54246384}\n",
      "16000/16000 [==============================] - 3s 168us/sample - loss: 0.6489 - auc_10: 0.6178 - acc: 0.6249 - f1: 0.5469 - val_loss: 0.6554 - val_auc_10: 0.6106 - val_acc: 0.6198 - val_f1: 0.5425\n",
      "Epoch 39/50\n",
      "15664/16000 [============================>.] - ETA: 0s - loss: 0.6487 - auc_10: 0.6178 - acc: 0.6248 - f1: 0.5463{'loss': 0.6489306495189667, 'auc_10': 0.61765116, 'acc': 0.62475, 'f1': 0.5460336, 'val_loss': 0.6554475634098053, 'val_auc_10': 0.61064196, 'val_acc': 0.61975, 'val_f1': 0.5419875}\n",
      "16000/16000 [==============================] - 3s 174us/sample - loss: 0.6489 - auc_10: 0.6177 - acc: 0.6248 - f1: 0.5460 - val_loss: 0.6554 - val_auc_10: 0.6106 - val_acc: 0.6198 - val_f1: 0.5420\n",
      "Epoch 40/50\n",
      "15808/16000 [============================>.] - ETA: 0s - loss: 0.6487 - auc_10: 0.6179 - acc: 0.6251 - f1: 0.5473{'loss': 0.6489385272562503, 'auc_10': 0.6176159, 'acc': 0.6245625, 'f1': 0.54712737, 'val_loss': 0.6554529099464417, 'val_auc_10': 0.61062473, 'val_acc': 0.62, 'val_f1': 0.5426714}\n",
      "16000/16000 [==============================] - 3s 182us/sample - loss: 0.6489 - auc_10: 0.6176 - acc: 0.6246 - f1: 0.5471 - val_loss: 0.6555 - val_auc_10: 0.6106 - val_acc: 0.6200 - val_f1: 0.5427\n",
      "Epoch 41/50\n",
      "15728/16000 [============================>.] - ETA: 0s - loss: 0.6490 - auc_10: 0.6176 - acc: 0.6251 - f1: 0.5465{'loss': 0.6489315203428269, 'auc_10': 0.6177809, 'acc': 0.625125, 'f1': 0.5471636, 'val_loss': 0.6554284284114837, 'val_auc_10': 0.61063665, 'val_acc': 0.61975, 'val_f1': 0.54145646}\n",
      "16000/16000 [==============================] - 3s 178us/sample - loss: 0.6489 - auc_10: 0.6178 - acc: 0.6251 - f1: 0.5472 - val_loss: 0.6554 - val_auc_10: 0.6106 - val_acc: 0.6198 - val_f1: 0.5415\n",
      "Epoch 42/50\n",
      "15904/16000 [============================>.] - ETA: 0s - loss: 0.6489 - auc_10: 0.6177 - acc: 0.6246 - f1: 0.5465{'loss': 0.6489270622134209, 'auc_10': 0.61772275, 'acc': 0.6245, 'f1': 0.5466211, 'val_loss': 0.6554167214632034, 'val_auc_10': 0.6105282, 'val_acc': 0.6195, 'val_f1': 0.54161584}\n",
      "16000/16000 [==============================] - 3s 210us/sample - loss: 0.6489 - auc_10: 0.6177 - acc: 0.6245 - f1: 0.5466 - val_loss: 0.6554 - val_auc_10: 0.6105 - val_acc: 0.6195 - val_f1: 0.5416\n",
      "Epoch 43/50\n",
      "15728/16000 [============================>.] - ETA: 0s - loss: 0.6494 - auc_10: 0.6168 - acc: 0.6237 - f1: 0.5468{'loss': 0.648923134714365, 'auc_10': 0.6176634, 'acc': 0.6244375, 'f1': 0.5467264, 'val_loss': 0.655428288936615, 'val_auc_10': 0.6105989, 'val_acc': 0.61975, 'val_f1': 0.5418093}\n",
      "16000/16000 [==============================] - 3s 201us/sample - loss: 0.6489 - auc_10: 0.6177 - acc: 0.6244 - f1: 0.5467 - val_loss: 0.6554 - val_auc_10: 0.6106 - val_acc: 0.6198 - val_f1: 0.5418\n",
      "Epoch 44/50\n",
      "15808/16000 [============================>.] - ETA: 0s - loss: 0.6488 - auc_10: 0.6178 - acc: 0.6248 - f1: 0.5465{'loss': 0.6489222589731216, 'auc_10': 0.61771286, 'acc': 0.6246875, 'f1': 0.5460792, 'val_loss': 0.655435008764267, 'val_auc_10': 0.6106028, 'val_acc': 0.61975, 'val_f1': 0.5426923}\n",
      "16000/16000 [==============================] - 3s 176us/sample - loss: 0.6489 - auc_10: 0.6177 - acc: 0.6247 - f1: 0.5461 - val_loss: 0.6554 - val_auc_10: 0.6106 - val_acc: 0.6198 - val_f1: 0.5427\n",
      "Epoch 45/50\n",
      "15984/16000 [============================>.] - ETA: 0s - loss: 0.6489 - auc_10: 0.6177 - acc: 0.6246 - f1: 0.5448{'loss': 0.6489184437394142, 'auc_10': 0.61764324, 'acc': 0.6245, 'f1': 0.5447267, 'val_loss': 0.6554403229951858, 'val_auc_10': 0.61062586, 'val_acc': 0.62, 'val_f1': 0.54192364}\n",
      "16000/16000 [==============================] - 3s 176us/sample - loss: 0.6489 - auc_10: 0.6176 - acc: 0.6245 - f1: 0.5447 - val_loss: 0.6554 - val_auc_10: 0.6106 - val_acc: 0.6200 - val_f1: 0.5419\n",
      "Epoch 46/50\n",
      "15984/16000 [============================>.] - ETA: 0s - loss: 0.6489 - auc_10: 0.6177 - acc: 0.6248 - f1: 0.5463{'loss': 0.648918648660183, 'auc_10': 0.61765933, 'acc': 0.6246875, 'f1': 0.54638815, 'val_loss': 0.6554425067901611, 'val_auc_10': 0.61066186, 'val_acc': 0.61975, 'val_f1': 0.54301906}\n",
      "16000/16000 [==============================] - 3s 178us/sample - loss: 0.6489 - auc_10: 0.6177 - acc: 0.6247 - f1: 0.5464 - val_loss: 0.6554 - val_auc_10: 0.6107 - val_acc: 0.6198 - val_f1: 0.5430\n",
      "Epoch 47/50\n",
      "15808/16000 [============================>.] - ETA: 0s - loss: 0.6490 - auc_10: 0.6177 - acc: 0.6242 - f1: 0.5477{'loss': 0.6489123506546021, 'auc_10': 0.61780816, 'acc': 0.6244375, 'f1': 0.54740465, 'val_loss': 0.6554322535991669, 'val_auc_10': 0.6106838, 'val_acc': 0.61975, 'val_f1': 0.54181314}\n",
      "16000/16000 [==============================] - 3s 172us/sample - loss: 0.6489 - auc_10: 0.6178 - acc: 0.6244 - f1: 0.5474 - val_loss: 0.6554 - val_auc_10: 0.6107 - val_acc: 0.6198 - val_f1: 0.5418\n",
      "Epoch 48/50\n",
      "15696/16000 [============================>.] - ETA: 0s - loss: 0.6491 - auc_10: 0.6174 - acc: 0.6244 - f1: 0.5462{'loss': 0.6489113604724407, 'auc_10': 0.6176801, 'acc': 0.6245, 'f1': 0.5464846, 'val_loss': 0.6554361391067505, 'val_auc_10': 0.6106738, 'val_acc': 0.6195, 'val_f1': 0.5424196}\n",
      "16000/16000 [==============================] - 3s 171us/sample - loss: 0.6489 - auc_10: 0.6177 - acc: 0.6245 - f1: 0.5465 - val_loss: 0.6554 - val_auc_10: 0.6107 - val_acc: 0.6195 - val_f1: 0.5424\n",
      "Epoch 49/50\n",
      "15696/16000 [============================>.] - ETA: 0s - loss: 0.6495 - auc_10: 0.6168 - acc: 0.6230 - f1: 0.5470{'loss': 0.648906850785017, 'auc_10': 0.6177215, 'acc': 0.6241875, 'f1': 0.5466964, 'val_loss': 0.6554357600212097, 'val_auc_10': 0.61066294, 'val_acc': 0.61975, 'val_f1': 0.54206043}\n",
      "16000/16000 [==============================] - 3s 201us/sample - loss: 0.6489 - auc_10: 0.6177 - acc: 0.6242 - f1: 0.5467 - val_loss: 0.6554 - val_auc_10: 0.6107 - val_acc: 0.6198 - val_f1: 0.5421\n",
      "Epoch 50/50\n",
      "15808/16000 [============================>.] - ETA: 0s - loss: 0.6484 - auc_10: 0.6184 - acc: 0.6254 - f1: 0.5482- ETA: 2s - loss: 0.64{'loss': 0.6489063556194306, 'auc_10': 0.6177596, 'acc': 0.6245625, 'f1': 0.54759765, 'val_loss': 0.6554486086368561, 'val_auc_10': 0.6107509, 'val_acc': 0.62025, 'val_f1': 0.5435572}\n",
      "16000/16000 [==============================] - 4s 220us/sample - loss: 0.6489 - auc_10: 0.6178 - acc: 0.6246 - f1: 0.5476 - val_loss: 0.6554 - val_auc_10: 0.6108 - val_acc: 0.6202 - val_f1: 0.5436\n"
     ]
    }
   ],
   "source": [
    "model1.compile(optimizer,\n",
    "#       loss=\"binary_crossentropy\",\n",
    "      loss = \"categorical_crossentropy\",\n",
    "      metrics=['AUC', 'accuracy', f1])\n",
    "\n",
    "\n",
    "model1.fit(X_train,y_train,epochs=50, validation_data=(X_test,y_test), batch_size=16, \n",
    "          callbacks=[history_own, checkpoint, \n",
    "                     lrate, \n",
    "#                      earlystop, \n",
    "#                      tensorboard_callback\n",
    "                    ]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_own.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 14412), started 0:26:47 ago. (Use '!kill 14412' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:6006\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1717fe84448>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
