{"cells":[{"cell_type":"markdown","metadata":{"id":"YXgZMy8N_ZI9"},"source":["## Tweets topic classification"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1031,"status":"ok","timestamp":1607265536579,"user":{"displayName":"Uday Lunawat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhLxNKOE1oE8oHaKBBVBxxiaZSs6dw131rgCEu_Qpo=s64","userId":"03427549702697233924"},"user_tz":-330},"id":"dQoLmOPL8f4x","outputId":"1b89f6fe-f5a8-4ed2-a0ae-6e78bf10d76c"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/tweets_topics\n"]}],"source":["%cd tweets_topics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nw1sG_lqVE71"},"outputs":[],"source":["# Importing NLP tools\n","import re, nltk, glob, random, pickle\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.classify.scikitlearn import SklearnClassifier\n","\n","# Importing Machine Learning Models\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.svm import LinearSVC\n","from sklearn.linear_model import SGDClassifier\n","\n","from helper import get_features\n","from config import save_features, save_model, save_model_dir, output_dir, inference_dir\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9fLoTEraiABA"},"outputs":[],"source":[" \n","import warnings\n","warnings.filterwarnings(\"ignore\")\n"," \n","nltk.download('stopwords')\n","nltk.download('punkt')\n"," \n","filenames = glob.glob(\"train_data/*.txt\")\n","print(\"\\nList of categories:\\n\")\n","[print(file) for file in filenames]\n"," \n","labelled_tweets = []\n","all_words = []\n","stpwords = stopwords.words('english')\n","for filename in filenames:\n","    file = open(filename, encoding='utf-8').read()\n","    for tweet in file.split('\\n'): # splitting text file in each line\n"," \n","        tweet = re.sub(r'[^\\w\\s]', '', tweet)\n","        # ^ within a set ([]) will exclude the expression class to it's right\n","        #  \\w Matches alphanumeric characters, which means a-z, A-Z, and 0-9. It also matches the underscore, _.\n","        #  \\s Matches non-whitespace characters.\n","        # Hence all characters except \\w and \\s will be removed\n"," \n","        tweet = re.sub(\" \\d+\", \" \", tweet)\n","        # replaces \\d (digits) with space\n"," \n","        tweet = [i.lower() for i in list(set(word_tokenize(tweet)) - set(stpwords))]\n","        # word_tokenize - returns a tokenized copy of text\n","        # Hence, list(set(word_tokenize(tweet)) - set(stpwords)) is removing stop words from tokenized copy of text and lower() sets them to lowercase\n"," \n","        all_words += tweet # adding pre-processed tweets to all_words list \n","        labelled_tweets.append((tweet, filename[5:-4]))  # extract category name from filename\n"," \n","# shuffling list of tweet keywords\n","random.shuffle(labelled_tweets)\n"," \n","word_features = list(all_words)\n"," \n","print(\"Generating features from data!\")\n","# contains a set of features generated using train data with boolean values whether the tweet contains it or not\n","feature_set = [(get_features(text), label) for (text, label) in labelled_tweets]\n"," \n","n = 500\n","train_feature_set = feature_set[n:]\n","test_feature_set = feature_set[:n]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":201947,"status":"ok","timestamp":1607266583351,"user":{"displayName":"Uday Lunawat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhLxNKOE1oE8oHaKBBVBxxiaZSs6dw131rgCEu_Qpo=s64","userId":"03427549702697233924"},"user_tz":-330},"id":"4CKHEf5NeQZg","outputId":"f5f51a5a-2b8c-4bc1-e1cf-dce6cbf42f97"},"outputs":[{"name":"stdout","output_type":"stream","text":["Saving Features!\n","Training Linear_SVC...\n","Linear_SVC is 74.2% accurate.\n","\n","Saving Trained Linear_SVC model.\n","Training Naive_Bayes...\n","Naive_Bayes is 72.39999999999999% accurate.\n","\n","Saving Trained Naive_Bayes model.\n","Training Logistic_Regression...\n","Logistic_Regression is 75.6% accurate.\n","\n","Saving Trained Logistic_Regression model.\n","Training Multinomial_Naive_Bayes...\n","Multinomial_Naive_Bayes is 75.0% accurate.\n","\n","Saving Trained Multinomial_Naive_Bayes model.\n","Training SGD_Classifier...\n","SGD_Classifier is 75.6% accurate.\n","\n","Saving Trained SGD_Classifier model.\n"]}],"source":["\n","# Dictionary of classifiers\n","classifier_dict = {'Linear_SVC':SklearnClassifier(LinearSVC()),\n","                   'Naive_Bayes':nltk.NaiveBayesClassifier,\n","                   'Logistic_Regression':SklearnClassifier(LogisticRegression(multi_class='ovr')),  # one-vs-rest\n","                   'Multinomial_Naive_Bayes':SklearnClassifier(MultinomialNB()),\n","                   'SGD_Classifier':SklearnClassifier(SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, random_state=42, max_iter=5, tol=None))\n","                   }\n","\n","# save features\n","if save_features == True:\n","    print(\"Saving Features!\")\n","    save_word_features = open(save_model_dir+\"word_features.pickle\", \"wb\")\n","    pickle.dump(word_features, save_word_features)\n","    save_word_features.close()\n","\n","# training and saving models\n","for classifier_name, classifier_object in classifier_dict.items():\n","    classifier = classifier_object.train(train_feature_set)\n","    print(\"Training {}...\".format(classifier_name))\n","    print(\"{} is {}% accurate.\\n\".format(classifier_name, \\\n","                                        (nltk.classify.accuracy(classifier, test_feature_set)) * 100))\n","    if save_model == True:\n","        print(\"Saving Trained {} model.\".format(classifier_name))\n","        classifier_file = open(\"{}.pickle\".format(save_model_dir+classifier_name), \"wb\")\n","        pickle.dump(classifier, classifier_file)\n","        classifier_file.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2620,"status":"ok","timestamp":1607265187381,"user":{"displayName":"Uday Lunawat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhLxNKOE1oE8oHaKBBVBxxiaZSs6dw131rgCEu_Qpo=s64","userId":"03427549702697233924"},"user_tz":-330},"id":"EQ_VhziSX3rO","outputId":"de536555-324a-4611-c5bc-030dd6d036cc"},"outputs":[{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","\n","List of categories:\n","\n","Traceback (most recent call last):\n","  File \"train.py\", line 88, in \u003cmodule\u003e\n","    classifier = classifier_object.train(train_feature_set)\n","  File \"/usr/local/lib/python3.6/dist-packages/nltk/classify/scikitlearn.py\", line 116, in train\n","    X, y = list(zip(*labeled_featuresets))\n","ValueError: not enough values to unpack (expected 2, got 0)\n"]}],"source":["# !python train.py"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1255,"status":"ok","timestamp":1607257906838,"user":{"displayName":"Uday Lunawat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhLxNKOE1oE8oHaKBBVBxxiaZSs6dw131rgCEu_Qpo=s64","userId":"03427549702697233924"},"user_tz":-330},"id":"d87IrwWjBj6u","outputId":"0390bac3-4c00-4f28-9aed-439c3fda0c9f"},"outputs":[{"name":"stdout","output_type":"stream","text":["business\n","business\n","politics\n","politics\n","technology\n","sports\n"]}],"source":["# prediction and producing output.txt\n","import pickle\n","import nltk\n","import pandas as pd\n","from helper import get_features\n","\n","classifier = pickle.load(open('model/MNB.pickle', 'rb'))\n","word_features = pickle.load(open('model/word_features.pickle', 'rb'))\n","\n","def predict_topic(tweet_text):\n","    tweet = nltk.word_tokenize(tweet_text.lower())\n","    return classifier.classify(get_features(tweet))\n","\n","\n","tweet_data = pd.read_csv('60tweets.csv')\n","# tweet_data.head()\n","\n","output = open(output_dir+\"output.txt\", \"w+\")\n","for index, row in tweet_data.iterrows():\n","    output.write(\"{} {}\\n\".format(row.id, predict_topic(row.tweets)))\n","output.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1353,"status":"ok","timestamp":1607264710500,"user":{"displayName":"Uday Lunawat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhLxNKOE1oE8oHaKBBVBxxiaZSs6dw131rgCEu_Qpo=s64","userId":"03427549702697233924"},"user_tz":-330},"id":"Fgu5iA1jcMPV","outputId":"0a37bfad-b0ee-48cb-918a-1bb9d0fe060d"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content\n","/content/tweets_topics\n"]}],"source":["%cd /content\n","!mkdir tweets_topics\n","%cd tweets_topics"]},{"cell_type":"markdown","metadata":{"id":"eLUXViQ97XGV"},"source":["## Data download"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2354,"status":"ok","timestamp":1607649431561,"user":{"displayName":"Uday Lunawat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhLxNKOE1oE8oHaKBBVBxxiaZSs6dw131rgCEu_Qpo=s64","userId":"03427549702697233924"},"user_tz":-330},"id":"08id_En1Xj5D","outputId":"ae4ec234-6e7f-4e9d-c480-4d8d91ad4570"},"outputs":[{"name":"stdout","output_type":"stream","text":["--2020-12-11 01:17:29--  https://www.dropbox.com/s/xg4lred37b558el/stopwords.txt\n","Resolving www.dropbox.com (www.dropbox.com)... 162.125.5.18, 2620:100:601d:18::a27d:512\n","Connecting to www.dropbox.com (www.dropbox.com)|162.125.5.18|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: /s/raw/xg4lred37b558el/stopwords.txt [following]\n","--2020-12-11 01:17:29--  https://www.dropbox.com/s/raw/xg4lred37b558el/stopwords.txt\n","Reusing existing connection to www.dropbox.com:443.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://uc1163e22b45ebc1e97710c51a0c.dl.dropboxusercontent.com/cd/0/inline/BE3W99apAmqHgO71_7J3NyYUSjkvj1w2GMC6FJkNKTt5vT0cBgknUsa1oVQF6yySMo8rknvc5py5XGeq-iDDaR3t-ejvoLIkzKve6NOR-27otd78KBRNHonUcvJ6wjQChbw/file# [following]\n","--2020-12-11 01:17:30--  https://uc1163e22b45ebc1e97710c51a0c.dl.dropboxusercontent.com/cd/0/inline/BE3W99apAmqHgO71_7J3NyYUSjkvj1w2GMC6FJkNKTt5vT0cBgknUsa1oVQF6yySMo8rknvc5py5XGeq-iDDaR3t-ejvoLIkzKve6NOR-27otd78KBRNHonUcvJ6wjQChbw/file\n","Resolving uc1163e22b45ebc1e97710c51a0c.dl.dropboxusercontent.com (uc1163e22b45ebc1e97710c51a0c.dl.dropboxusercontent.com)... 162.125.5.15, 2620:100:601d:15::a27d:50f\n","Connecting to uc1163e22b45ebc1e97710c51a0c.dl.dropboxusercontent.com (uc1163e22b45ebc1e97710c51a0c.dl.dropboxusercontent.com)|162.125.5.15|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 622 [text/plain]\n","Saving to: ‘stopwords.txt’\n","\n","stopwords.txt       100%[===================\u003e]     622  --.-KB/s    in 0s      \n","\n","2020-12-11 01:17:30 (96.1 MB/s) - ‘stopwords.txt’ saved [622/622]\n","\n"]}],"source":["!wget https://www.dropbox.com/s/xg4lred37b558el/stopwords.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11480,"status":"ok","timestamp":1607648956551,"user":{"displayName":"Uday Lunawat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhLxNKOE1oE8oHaKBBVBxxiaZSs6dw131rgCEu_Qpo=s64","userId":"03427549702697233924"},"user_tz":-330},"id":"i10hsf3HlI-k","outputId":"5ec07c98-6f8c-4256-ae2a-b94dc5ef7320"},"outputs":[{"name":"stdout","output_type":"stream","text":["--2020-12-11 01:09:25--  https://www.dropbox.com/s/7qc0hfq61ovabdp/60tweets.csv\n","Resolving www.dropbox.com (www.dropbox.com)... 162.125.5.18, 2620:100:601d:18::a27d:512\n","Connecting to www.dropbox.com (www.dropbox.com)|162.125.5.18|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: /s/raw/7qc0hfq61ovabdp/60tweets.csv [following]\n","--2020-12-11 01:09:25--  https://www.dropbox.com/s/raw/7qc0hfq61ovabdp/60tweets.csv\n","Reusing existing connection to www.dropbox.com:443.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://uc764485e035e7fa578f3c45016d.dl.dropboxusercontent.com/cd/0/inline/BE2nYAERNp8Im7GBEdCA_VtHcJmUwvciyhGe96_rsvfHFg946-X-LoXW61KUEBiZHhqBjXrG96Hok-HqYO90LEq4VBSi2VJVtWjeEGQnPRqg0vPal7gGGJ1tyKStS80D1UY/file# [following]\n","--2020-12-11 01:09:26--  https://uc764485e035e7fa578f3c45016d.dl.dropboxusercontent.com/cd/0/inline/BE2nYAERNp8Im7GBEdCA_VtHcJmUwvciyhGe96_rsvfHFg946-X-LoXW61KUEBiZHhqBjXrG96Hok-HqYO90LEq4VBSi2VJVtWjeEGQnPRqg0vPal7gGGJ1tyKStS80D1UY/file\n","Resolving uc764485e035e7fa578f3c45016d.dl.dropboxusercontent.com (uc764485e035e7fa578f3c45016d.dl.dropboxusercontent.com)... 162.125.5.15, 2620:100:601d:15::a27d:50f\n","Connecting to uc764485e035e7fa578f3c45016d.dl.dropboxusercontent.com (uc764485e035e7fa578f3c45016d.dl.dropboxusercontent.com)|162.125.5.15|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 8387 (8.2K) [text/plain]\n","Saving to: ‘60tweets.csv’\n","\n","60tweets.csv        100%[===================\u003e]   8.19K  --.-KB/s    in 0s      \n","\n","2020-12-11 01:09:26 (177 MB/s) - ‘60tweets.csv’ saved [8387/8387]\n","\n","--2020-12-11 01:09:26--  https://www.dropbox.com/s/dc76asx6a845sdt/datasets.zip\n","Resolving www.dropbox.com (www.dropbox.com)... 162.125.5.18, 2620:100:601d:18::a27d:512\n","Connecting to www.dropbox.com (www.dropbox.com)|162.125.5.18|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: /s/raw/dc76asx6a845sdt/datasets.zip [following]\n","--2020-12-11 01:09:27--  https://www.dropbox.com/s/raw/dc76asx6a845sdt/datasets.zip\n","Reusing existing connection to www.dropbox.com:443.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://uc937d3d880e5dec12d3050fe174.dl.dropboxusercontent.com/cd/0/inline/BE0BXH6PxF-x9xvbUPZM4WejQhyY1fVlNEfl2JeMmp6twATfoqgekvJFr_FFXEelQ1Vxflr7IltUA4vdBnRHVoX63YG5Efla4wOmYDc5g2uocdThP-HRICteRdh2LlVpfyM/file# [following]\n","--2020-12-11 01:09:27--  https://uc937d3d880e5dec12d3050fe174.dl.dropboxusercontent.com/cd/0/inline/BE0BXH6PxF-x9xvbUPZM4WejQhyY1fVlNEfl2JeMmp6twATfoqgekvJFr_FFXEelQ1Vxflr7IltUA4vdBnRHVoX63YG5Efla4wOmYDc5g2uocdThP-HRICteRdh2LlVpfyM/file\n","Resolving uc937d3d880e5dec12d3050fe174.dl.dropboxusercontent.com (uc937d3d880e5dec12d3050fe174.dl.dropboxusercontent.com)... 162.125.5.15, 2620:100:601d:15::a27d:50f\n","Connecting to uc937d3d880e5dec12d3050fe174.dl.dropboxusercontent.com (uc937d3d880e5dec12d3050fe174.dl.dropboxusercontent.com)|162.125.5.15|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: /cd/0/inline2/BE2wEEWdk31VWyXp0mDYy6MKegs9qnOVoti9R0tC-u932qX2wK_wzCXajOO_CT-G17-oTkGv31k3mIeGhHACqWT6ubbCAIFv--TsHEmaqLNGibyBYSA-e9LFQ3Ld1o_1LgSrtjjtvp2Ibts_o-ihRRGZImbiKBf6EsJufz7QM0mt-JcA2fG_vXUTSKJ-nLlhGIsu4IE_nUnNpADXUhY698o-PMqMhFtJFiop40N0eARzJU6LcDve0r0n2txJUHd5HoqBnZ6lF-5na_GRB7mIbiQuXRyMG1QHS5eSD64KZBxhhcoecgah7Vf9vDtfChzwkDOr1xg3pvapvtslOY4FavRlVkNjrR4QmTjiy9jDamhbYw/file [following]\n","--2020-12-11 01:09:28--  https://uc937d3d880e5dec12d3050fe174.dl.dropboxusercontent.com/cd/0/inline2/BE2wEEWdk31VWyXp0mDYy6MKegs9qnOVoti9R0tC-u932qX2wK_wzCXajOO_CT-G17-oTkGv31k3mIeGhHACqWT6ubbCAIFv--TsHEmaqLNGibyBYSA-e9LFQ3Ld1o_1LgSrtjjtvp2Ibts_o-ihRRGZImbiKBf6EsJufz7QM0mt-JcA2fG_vXUTSKJ-nLlhGIsu4IE_nUnNpADXUhY698o-PMqMhFtJFiop40N0eARzJU6LcDve0r0n2txJUHd5HoqBnZ6lF-5na_GRB7mIbiQuXRyMG1QHS5eSD64KZBxhhcoecgah7Vf9vDtfChzwkDOr1xg3pvapvtslOY4FavRlVkNjrR4QmTjiy9jDamhbYw/file\n","Reusing existing connection to uc937d3d880e5dec12d3050fe174.dl.dropboxusercontent.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 103774153 (99M) [application/zip]\n","Saving to: ‘datasets.zip’\n","\n","datasets.zip        100%[===================\u003e]  98.97M  58.7MB/s    in 1.7s    \n","\n","2020-12-11 01:09:30 (58.7 MB/s) - ‘datasets.zip’ saved [103774153/103774153]\n","\n","--2020-12-11 01:09:30--  https://www.dropbox.com/s/gcxs864fz93f1d4/utilities.py\n","Resolving www.dropbox.com (www.dropbox.com)... 162.125.5.18, 2620:100:601d:18::a27d:512\n","Connecting to www.dropbox.com (www.dropbox.com)|162.125.5.18|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: /s/raw/gcxs864fz93f1d4/utilities.py [following]\n","--2020-12-11 01:09:31--  https://www.dropbox.com/s/raw/gcxs864fz93f1d4/utilities.py\n","Reusing existing connection to www.dropbox.com:443.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://uc48ccf0e5cc5c1d582cf8dc7390.dl.dropboxusercontent.com/cd/0/inline/BE3ghUqE_f6lNGMYkk8UrU8-PPiZi44yqJdSMsu4JfAxDDAIzzklaBi-w_qT_lhS-fSeFHp3Ncjo7W_PpvO56-eLizWXh3mndiEodJ1tU0mIvnIeN4hhmj2ty2PrOCL3b4o/file# [following]\n","--2020-12-11 01:09:31--  https://uc48ccf0e5cc5c1d582cf8dc7390.dl.dropboxusercontent.com/cd/0/inline/BE3ghUqE_f6lNGMYkk8UrU8-PPiZi44yqJdSMsu4JfAxDDAIzzklaBi-w_qT_lhS-fSeFHp3Ncjo7W_PpvO56-eLizWXh3mndiEodJ1tU0mIvnIeN4hhmj2ty2PrOCL3b4o/file\n","Resolving uc48ccf0e5cc5c1d582cf8dc7390.dl.dropboxusercontent.com (uc48ccf0e5cc5c1d582cf8dc7390.dl.dropboxusercontent.com)... 162.125.5.15, 2620:100:601d:15::a27d:50f\n","Connecting to uc48ccf0e5cc5c1d582cf8dc7390.dl.dropboxusercontent.com (uc48ccf0e5cc5c1d582cf8dc7390.dl.dropboxusercontent.com)|162.125.5.15|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 495 [text/plain]\n","Saving to: ‘utilities.py’\n","\n","utilities.py        100%[===================\u003e]     495  --.-KB/s    in 0s      \n","\n","2020-12-11 01:09:32 (59.7 MB/s) - ‘utilities.py’ saved [495/495]\n","\n","Archive:  datasets.zip\n","  inflating: datasets/dev_set.csv    \n","  inflating: datasets/__MACOSX/._dev_set.csv  \n","  inflating: datasets/test_set.csv   \n","  inflating: datasets/__MACOSX/._test_set.csv  \n","  inflating: datasets/training_set.csv  \n","  inflating: datasets/__MACOSX/._training_set.csv  \n"]}],"source":["!wget https://www.dropbox.com/s/7qc0hfq61ovabdp/60tweets.csv\n","!wget https://www.dropbox.com/s/dc76asx6a845sdt/datasets.zip\n","!wget https://www.dropbox.com/s/gcxs864fz93f1d4/utilities.py\n","!unzip datasets.zip -d datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10666,"status":"ok","timestamp":1607648965096,"user":{"displayName":"Uday Lunawat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhLxNKOE1oE8oHaKBBVBxxiaZSs6dw131rgCEu_Qpo=s64","userId":"03427549702697233924"},"user_tz":-330},"id":"ElvfxughKWdO","outputId":"312137c3-beb2-48d6-cf10-70775616286d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting git+https://github.com/LIAAD/yake\n","  Cloning https://github.com/LIAAD/yake to /tmp/pip-req-build-5do9hpsa\n","  Running command git clone -q https://github.com/LIAAD/yake /tmp/pip-req-build-5do9hpsa\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from yake==0.4.3) (0.8.7)\n","Requirement already satisfied: click\u003e=6.0 in /usr/local/lib/python3.6/dist-packages (from yake==0.4.3) (7.1.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from yake==0.4.3) (1.18.5)\n","Collecting segtok\n","  Downloading https://files.pythonhosted.org/packages/41/08/582dab5f4b1d5ca23bc6927b4bb977c8ff7f3a87a3b98844ef833e2f5623/segtok-1.5.10.tar.gz\n","Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from yake==0.4.3) (2.5)\n","Collecting jellyfish\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/09/927ae35fc5a9f70abb6cc2c27ee88fc48549f7bc4786c1d4b177c22e997d/jellyfish-0.8.2-cp36-cp36m-manylinux2014_x86_64.whl (93kB)\n","\u001b[K     |████████████████████████████████| 102kB 4.6MB/s \n","\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from segtok-\u003eyake==0.4.3) (2019.12.20)\n","Requirement already satisfied: decorator\u003e=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx-\u003eyake==0.4.3) (4.4.2)\n","Building wheels for collected packages: yake, segtok\n","  Building wheel for yake (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for yake: filename=yake-0.4.3-py2.py3-none-any.whl size=66280 sha256=ae1542617908f40dac565c3a7b76c3e8d2d8982a064163c8f8a76119d6f56ab3\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-3zkvhbci/wheels/be/35/27/e4ebd54b78c1806ed8b0271ce247fcd91e2bedde35889fbc9b\n","  Building wheel for segtok (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for segtok: filename=segtok-1.5.10-cp36-none-any.whl size=25020 sha256=958ce424bdbd3cda6f01a968b7fff8bd3366ad941512e6bc9b60580698240bb2\n","  Stored in directory: /root/.cache/pip/wheels/b4/39/f6/9ca1c5cabde964d728023b5751c3a206a5c8cc40252321fb6b\n","Successfully built yake segtok\n","Installing collected packages: segtok, jellyfish, yake\n","Successfully installed jellyfish-0.8.2 segtok-1.5.10 yake-0.4.3\n"]}],"source":["!pip install git+https://github.com/LIAAD/yake"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16682,"status":"ok","timestamp":1607648972419,"user":{"displayName":"Uday Lunawat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhLxNKOE1oE8oHaKBBVBxxiaZSs6dw131rgCEu_Qpo=s64","userId":"03427549702697233924"},"user_tz":-330},"id":"5lJRxduAyhVQ","outputId":"7f91d8c6-404e-4071-f7ef-e06c2ff4a7d4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n","Requirement already satisfied: spacy\u003e=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n","Requirement already satisfied: plac\u003c1.2.0,\u003e=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy\u003e=2.2.2-\u003een_core_web_sm==2.2.5) (1.1.3)\n","Requirement already satisfied: wasabi\u003c1.1.0,\u003e=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy\u003e=2.2.2-\u003een_core_web_sm==2.2.5) (0.8.0)\n","Requirement already satisfied: preshed\u003c3.1.0,\u003e=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy\u003e=2.2.2-\u003een_core_web_sm==2.2.5) (3.0.5)\n","Requirement already satisfied: tqdm\u003c5.0.0,\u003e=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy\u003e=2.2.2-\u003een_core_web_sm==2.2.5) (4.41.1)\n","Requirement already satisfied: blis\u003c0.5.0,\u003e=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy\u003e=2.2.2-\u003een_core_web_sm==2.2.5) (0.4.1)\n","Requirement already satisfied: murmurhash\u003c1.1.0,\u003e=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy\u003e=2.2.2-\u003een_core_web_sm==2.2.5) (1.0.5)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy\u003e=2.2.2-\u003een_core_web_sm==2.2.5) (7.4.0)\n","Requirement already satisfied: srsly\u003c1.1.0,\u003e=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy\u003e=2.2.2-\u003een_core_web_sm==2.2.5) (1.0.5)\n","Requirement already satisfied: numpy\u003e=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy\u003e=2.2.2-\u003een_core_web_sm==2.2.5) (1.18.5)\n","Requirement already satisfied: requests\u003c3.0.0,\u003e=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy\u003e=2.2.2-\u003een_core_web_sm==2.2.5) (2.23.0)\n","Requirement already satisfied: catalogue\u003c1.1.0,\u003e=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy\u003e=2.2.2-\u003een_core_web_sm==2.2.5) (1.0.0)\n","Requirement already satisfied: cymem\u003c2.1.0,\u003e=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy\u003e=2.2.2-\u003een_core_web_sm==2.2.5) (2.0.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy\u003e=2.2.2-\u003een_core_web_sm==2.2.5) (50.3.2)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,\u003c1.26,\u003e=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests\u003c3.0.0,\u003e=2.13.0-\u003espacy\u003e=2.2.2-\u003een_core_web_sm==2.2.5) (1.24.3)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests\u003c3.0.0,\u003e=2.13.0-\u003espacy\u003e=2.2.2-\u003een_core_web_sm==2.2.5) (2020.12.5)\n","Requirement already satisfied: idna\u003c3,\u003e=2.5 in /usr/local/lib/python3.6/dist-packages (from requests\u003c3.0.0,\u003e=2.13.0-\u003espacy\u003e=2.2.2-\u003een_core_web_sm==2.2.5) (2.10)\n","Requirement already satisfied: chardet\u003c4,\u003e=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests\u003c3.0.0,\u003e=2.13.0-\u003espacy\u003e=2.2.2-\u003een_core_web_sm==2.2.5) (3.0.4)\n","Requirement already satisfied: importlib-metadata\u003e=0.20; python_version \u003c \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue\u003c1.1.0,\u003e=0.0.7-\u003espacy\u003e=2.2.2-\u003een_core_web_sm==2.2.5) (3.1.1)\n","Requirement already satisfied: zipp\u003e=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata\u003e=0.20; python_version \u003c \"3.8\"-\u003ecatalogue\u003c1.1.0,\u003e=0.0.7-\u003espacy\u003e=2.2.2-\u003een_core_web_sm==2.2.5) (3.4.0)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the model via spacy.load('en_core_web_sm')\n","\u001b[2K\u001b[38;5;2m✔ Loaded compatibility table\u001b[0m\n","\u001b[1m\n","====================== Installed models (spaCy v2.2.4) ======================\u001b[0m\n","\u001b[38;5;4mℹ spaCy installation: /usr/local/lib/python3.6/dist-packages/spacy\u001b[0m\n","\n","TYPE      NAME             MODEL            VERSION                            \n","package   en-core-web-sm   en_core_web_sm   \u001b[38;5;2m2.2.5\u001b[0m   \u001b[38;5;2m✔\u001b[0m\n","link      en               en_core_web_sm   \u001b[38;5;2m2.2.5\u001b[0m   \u001b[38;5;2m✔\u001b[0m\n","\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"data":{"text/plain":["True"]},"execution_count":3,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["! python -m spacy download en_core_web_sm\n","! python -m spacy validate\n","import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')"]},{"cell_type":"markdown","metadata":{"id":"yGXfqA9avr1B"},"source":["https://gist.githubusercontent.com/sebleier/554280/raw/7e0e4a1ce04c2bb7bd41089c9821dbcf6d0c786c/NLTK's%2520list%2520of%2520english%2520stopwords"]},{"cell_type":"markdown","metadata":{"id":"9pyOwMBf4Xu1"},"source":["https://github.com/kavgan/nlp-in-practice/blob/master/tf-idf/Keyword%20Extraction%20with%20TF-IDF%20and%20SKlearn.ipynb"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kPAeabaS1Ucq"},"outputs":[],"source":["input_file = 'datasets/training_set.csv'\r\n","input_type = 'train'"]},{"cell_type":"markdown","metadata":{"id":"xLJff3pw_cPV"},"source":["## Keyword extraction using RAKE"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4858,"status":"ok","timestamp":1607660591070,"user":{"displayName":"Uday Lunawat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhLxNKOE1oE8oHaKBBVBxxiaZSs6dw131rgCEu_Qpo=s64","userId":"03427549702697233924"},"user_tz":-330},"id":"lfvkmtVO70nu","outputId":"ed2c9f49-b3f0-4f94-bfdd-581f5a5ecec2"},"outputs":[{"data":{"text/plain":["1120000"]},"execution_count":23,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["import pandas as pd\r\n","tweet_data = pd.read_csv(input_file)\r\n","len(tweet_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":134},"executionInfo":{"elapsed":1336,"status":"ok","timestamp":1607660494081,"user":{"displayName":"Uday Lunawat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhLxNKOE1oE8oHaKBBVBxxiaZSs6dw131rgCEu_Qpo=s64","userId":"03427549702697233924"},"user_tz":-330},"id":"kkp2EcajbHb5","outputId":"1d73214c-8982-4d8e-839e-a1466f1b3bf8"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"25712391ac9f4e049f1f292c2521f291","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"]},"metadata":{"tags":[]},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","Creating yake__output.txt\n","Creating yake__output.csv\n","CPU times: user 208 ms, sys: 5.03 ms, total: 213 ms\n","Wall time: 254 ms\n"]}],"source":["%%time\r\n","model_name = 'yake'\r\n","import yake\r\n","import pandas as pd\r\n","tweet_data = pd.read_csv(input_file)\r\n","# tweet_data.head()\r\n","\r\n","# yake specific parameters\r\n","language = \"en\"\r\n","max_ngram_size = 2 # decrease the size if you want smaller keywords\r\n","deduplication_thresold = 0.9\r\n","deduplication_algo = 'seqm'\r\n","windowSize = 1\r\n","numOfKeywords = 1 # make sure to adjust no. of keywords\r\n","\r\n","custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_thresold, dedupFunc=deduplication_algo, windowsSize=windowSize, top=numOfKeywords, features=None)\r\n","\r\n","\r\n","\r\n","ids = []\r\n","keywords_list = []\r\n","\r\n","for index, row in tqdm(tweet_data.iterrows(), total=len(tweet_data)):\r\n","    keywords = custom_kw_extractor.extract_keywords(row.tweets)\r\n","    # print(index, keywords)\r\n","    ids.append(row.id)\r\n","    if keywords == []:\r\n","        keyword = 'nan'\r\n","    else:\r\n","        keyword = keywords[-1][-1]\r\n","    keywords_list.append(keyword)\r\n","\r\n","datatype = 'txt'\r\n","output_textfile = \"{}_{}_output.{}\".format(model_name, input_type, datatype)\r\n","output = open(output_textfile, \"w+\")\r\n","print(\"Creating \"+output_textfile)\r\n","for index, text in zip(ids, keywords_list):\r\n","    output.write(\"{} {}\\n\".format(index, text))\r\n","output.close()\r\n","\r\n","\r\n","datatype = 'csv'\r\n","output_textfile = \"{}_{}_output.{}\".format(model_name, input_type, datatype)\r\n","print(\"Creating \"+output_textfile)\r\n","tweet_keywords_df = pd.DataFrame(columns=['tweet_id','keyword'])\r\n","tweet_keywords_df['tweet_id'] = ids\r\n","tweet_keywords_df['keyword'] = keywords_list\r\n","tweet_keywords_df.to_csv(output_textfile)"]},{"cell_type":"markdown","metadata":{"id":"fZUK2lNhlAf9"},"source":[" ## keyword extraction using POS and pronoun extraction"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iZvGhdjQ_rj5"},"outputs":[],"source":["from tqdm.notebook import tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2234,"status":"ok","timestamp":1607660687640,"user":{"displayName":"Uday Lunawat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhLxNKOE1oE8oHaKBBVBxxiaZSs6dw131rgCEu_Qpo=s64","userId":"03427549702697233924"},"user_tz":-330},"id":"fm48FS-Hzxqb","outputId":"3b485c39-ea85-49cc-ed87-7d306c0566fe"},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: user 677 ms, sys: 38 ms, total: 715 ms\n","Wall time: 716 ms\n"]}],"source":["%%time\r\n","model_name = 'NLP_Base'\r\n","import re\r\n","import pandas as pd\r\n","from sklearn.feature_extraction.text import CountVectorizer\r\n","from sklearn.feature_extraction.text import TfidfTransformer\r\n","from utilities import pre_process, get_stop_words\r\n","\r\n","import re\r\n","import spacy\r\n","from collections import Counter\r\n","from string import punctuation\r\n","# nlp = spacy.load(\"en_core_web_sm\")\r\n","import en_core_web_sm\r\n","nlp = en_core_web_sm.load()\r\n","\r\n","#####################--Functions--#######################\r\n","\r\n","def get_hotwords(text):\r\n","    result = []\r\n","    pos_tag = ['PROPN','NOUN'] # 1 'ADJ', 'NOUN'\r\n","    doc = nlp(text.lower()) # 2\r\n","    for token in doc:\r\n","        # 3\r\n","        if(token.text in nlp.Defaults.stop_words or token.text in punctuation):\r\n","            continue\r\n","        # 4\r\n","        if(token.pos_ in pos_tag):\r\n","            result.append(token.text)\r\n","        \r\n","    return result # 5\r\n","\r\n","\r\n","def sort_coo(coo_matrix):\r\n","    tuples = zip(coo_matrix.col, coo_matrix.data)\r\n","    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\r\n","\r\n","def extract_topn_from_vector(feature_names, sorted_items, topn=10):\r\n","    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\r\n","    \r\n","    #use only topn items from vector\r\n","    sorted_items = sorted_items[:topn]\r\n","\r\n","    score_vals = []\r\n","    feature_vals = []\r\n","\r\n","    for idx, score in sorted_items:\r\n","        fname = feature_names[idx]\r\n","        \r\n","        #keep track of feature name and its corresponding score\r\n","        score_vals.append(round(score, 3))\r\n","        feature_vals.append(feature_names[idx])\r\n","\r\n","    #create a tuples of feature,score\r\n","    #results = zip(feature_vals,score_vals)\r\n","    results= {}\r\n","    for idx in range(len(feature_vals)):\r\n","        results[feature_vals[idx]]=score_vals[idx]\r\n","    \r\n","    return results\r\n","\r\n","def get_all_keywords(tweet):\r\n","    # get the document that we want to extract keywords from\r\n","    doc=tweet\r\n","\r\n","    #generate tf-idf for the given document\r\n","    tf_idf_vector=tfidf_transformer.transform(cv.transform([doc]))\r\n","\r\n","    #sort the tf-idf vectors by descending order of scores\r\n","    sorted_items=sort_coo(tf_idf_vector.tocoo())\r\n","\r\n","    #extract only the top n; n here is 10\r\n","    keywords=extract_topn_from_vector(feature_names,sorted_items,10)\r\n","\r\n","    return keywords\r\n","\r\n","def get_keyword(tweet):\r\n","    hotwords = get_hotwords(tweet)\r\n","\r\n","    # get the document that we want to extract keywords from\r\n","    doc=tweet\r\n","\r\n","    #generate tf-idf for the given document\r\n","    tf_idf_vector=tfidf_transformer.transform(cv.transform([doc]))\r\n","\r\n","    #sort the tf-idf vectors by descending order of scores\r\n","    sorted_items=sort_coo(tf_idf_vector.tocoo())\r\n","\r\n","    #extract only the top n; n here is 5\r\n","    keywords=extract_topn_from_vector(feature_names,sorted_items,5)\r\n","    final_keywords = dict()\r\n","    for key, value in keywords.items():\r\n","        if key in hotwords:\r\n","            # final_keywords_list.update({key,value})\r\n","            final_keywords[key]=value\r\n","\r\n","    if final_keywords == {}:\r\n","        if hotwords == []:\r\n","            return 'na'\r\n","        return hotwords[0]\r\n","\r\n","    keyword = max(final_keywords, key= lambda x: final_keywords[x])\r\n","    return keyword\r\n","\r\n","##########################--Main Program--###############################\r\n","\r\n","\r\n","# if __name__ == \"__main__\":\r\n","    \r\n","    # tweet_data = pd.read_csv(input_file)\r\n","    # df = tweet_data\r\n","    # df['tweets'] = df['tweets'].apply(lambda x:pre_process(x))\r\n","\r\n","    # # #load a set of stop words\r\n","    # stopwords=get_stop_words(\"stopwords.txt\")\r\n","\r\n","    # #get the text column \r\n","    # docs=df['tweets'].tolist()\r\n","\r\n","    # #create a vocabulary of words, \r\n","    # #ignore words that appear in 85% of documents, \r\n","    # #eliminate stop words\r\n","    # cv=CountVectorizer(max_df=0.85,stop_words=stopwords)\r\n","    # word_count_vector=cv.fit_transform(docs)\r\n","\r\n","    # tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\r\n","    # tfidf_transformer.fit(word_count_vector)\r\n","\r\n","    # # you only needs to do this once\r\n","    # feature_names=cv.get_feature_names()\r\n","\r\n","    # ids = []\r\n","    # keywords = []\r\n","\r\n","    # for index, row in tqdm(tweet_data.iterrows(), total = len(tweet_data)):\r\n","    #     keyword = get_keyword(row.tweets)\r\n","    #     ids.append(row.id)\r\n","    #     keywords.append(keyword)\r\n","    #     # print(row.id, keyword)\r\n","\r\n","    # datatype = 'txt'\r\n","    # output_textfile = \"{}_{}_output.{}\".format(model_name, input_type, datatype)\r\n","    # print(\"Creating \"+output_textfile)\r\n","    # output = open(output_textfile, \"w+\") # for text file\r\n","    # for index, text in zip(ids, keywords):\r\n","    #     output.write(\"{} {}\\n\".format(index, text))\r\n","    # output.close() # closing and saving text file\r\n","\r\n","    # datatype = 'csv'\r\n","    # output_textfile = \"{}_{}_output.{}\".format(model_name, input_type, datatype)\r\n","    # print(\"Creating \"+output_textfile)\r\n","    # tweet_keywords_df = pd.DataFrame(columns=['tweet_id','keyword'])\r\n","    # tweet_keywords_df['tweet_id'] = ids\r\n","    # tweet_keywords_df['keyword'] = keywords\r\n","    # tweet_keywords_df.to_csv(output_textfile)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"CWctA3cwCo0t"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"324e92fc0f364fe19ee34899cfff9f27","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Buffered data was truncated after reaching the output size limit."]}],"source":["%%time\r\n","ids = []\r\n","keywords = []\r\n","\r\n","for index, row in tqdm(tweet_data.iterrows(), total = len(tweet_data)):\r\n","    keyword = get_keyword(row.tweets)\r\n","    ids.append(row.id)\r\n","    keywords.append(keyword)\r\n","    # print(row.id, keyword)\r\n","\r\n","datatype = 'txt'\r\n","output_textfile = \"{}_{}_output.{}\".format(model_name, input_type, datatype)\r\n","print(\"Creating \"+output_textfile)\r\n","output = open(output_textfile, \"w+\") # for text file\r\n","for index, text in zip(ids, keywords):\r\n","    output.write(\"{} {}\\n\".format(index, text))\r\n","output.close() # closing and saving text file\r\n","\r\n","datatype = 'csv'\r\n","output_textfile = \"{}_{}_output.{}\".format(model_name, input_type, datatype)\r\n","print(\"Creating \"+output_textfile)\r\n","tweet_keywords_df = pd.DataFrame(columns=['tweet_id','keyword'])\r\n","tweet_keywords_df['tweet_id'] = ids\r\n","tweet_keywords_df['keyword'] = keywords\r\n","tweet_keywords_df.to_csv(output_textfile)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xk1hMnJd2HP6"},"outputs":[],"source":["# Play an audio beep. Any audio URL will do.\r\n","from google.colab import output\r\n","output.eval_js('new Audio(\"https://upload.wikimedia.org/wikipedia/commons/0/05/Beep-09.ogg\").play()')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x7eud6Pk9OHR"},"outputs":[],"source":["# increase speed by using this\r\n","%%timeit\r\n","tqdm.pandas()\r\n","tweet_data['keyword'] = tweet_data['tweets'].progress_apply(lambda row: custom_kw_extractor.extract_keywords(row)[-1][-1] \\\r\n","                                         if custom_kw_extractor.extract_keywords(row) != [] \\\r\n","                                         else 'nan')"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOcbPJmoGvEYm8h0PCOWI2c","collapsed_sections":["YXgZMy8N_ZI9"],"name":"Tweets Keyword Extractor Draft.ipynb","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"00efea729e044284ac7cf97eed7317ea":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"1fe833c9e94f4174a33ff8fd99cda782":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"25712391ac9f4e049f1f292c2521f291":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_575e5ba9dca44d68b4ab536050a7cbd8","IPY_MODEL_8e19cd1b2ae140ebaad15c19f24a783c"],"layout":"IPY_MODEL_3226b3c56fa04a52885827b3476c9620"}},"2c0ac2d3b24c4a7c946d94e62be35ba9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3226b3c56fa04a52885827b3476c9620":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"575e5ba9dca44d68b4ab536050a7cbd8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2c0ac2d3b24c4a7c946d94e62be35ba9","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_00efea729e044284ac7cf97eed7317ea","value":1}},"8e19cd1b2ae140ebaad15c19f24a783c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9416d8d2349d4d9b8056f5db14718a9c","placeholder":"​","style":"IPY_MODEL_1fe833c9e94f4174a33ff8fd99cda782","value":" 60/? [00:29\u0026lt;00:00,  2.01it/s]"}},"9416d8d2349d4d9b8056f5db14718a9c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}